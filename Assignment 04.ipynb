{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 04\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN? And a vector-to-sequence RNN?\n",
    "\n",
    "\n",
    "ANS = \n",
    "Applications are speech recognition, machine translation, image captioning and question answering.\n",
    "\n",
    "Sequence To Vector To Sequence RNN\n",
    "It is like a encoder-decoder model which first we have a temporal information and we condense it down to a vector which we can call it a “concept vector” that is a vector representing the concept of that expanded version then we expand that concept vector to the output that we want.\n",
    "\n",
    "\n",
    "Vector To Sequence RNN\n",
    "The RNN model takes a single vector as input and produces a sequence as output. An example of these models can be image to sentence model, which takes an image(consider it as a vector) and then produces a sentence to describe that image.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Why do people use encoder–decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n",
    "\n",
    "\n",
    "ANS = \n",
    "\n",
    "Because seq-2-seq RNNs translate one word at a time,encoder-decoder RNNs read & translate a sentence at a time.\n",
    "\n",
    "\n",
    "\n",
    "# How could you combine a convolutional neural network with an RNN to classify videos?\n",
    "\n",
    "\n",
    "ANS = \n",
    "\n",
    "steps:\n",
    "\n",
    "1.Run a frame from each second of video through a CNN\n",
    "\n",
    "2.Feed CNN outputs as input sequence to RNN\n",
    "\n",
    "3.Feed RNN outputs to softmax layer for probabilities of each class\n",
    "\n",
    "\n",
    "\n",
    "# What are the advantages of building an RNN using dynamic_rnn() rather than static_rnn()?\n",
    "\n",
    "\n",
    "ANS = \n",
    "\n",
    "1.avoids out-of-memory errors\n",
    "\n",
    "2.directly takes single tensor as input and output (covering all time steps)\n",
    "\n",
    "3.no need to stack, unstack, or transpose\n",
    "\n",
    "4.generates a smaller easier to visualize graph in TensorBoard\n",
    "\n",
    "\n",
    "# How can you deal with variable-length input sequences? What about variable-length output sequences?\n",
    "\n",
    "\n",
    "ANS = \n",
    "\n",
    "1 . set sequence_length parameter when calling static_rnn() or dynamic_rnn()\n",
    "\n",
    "2.pad smaller input/output to make them same size as largest input/output\n",
    "\n",
    "# What is a common way to distribute training and execution of a deep RNN across multiple GPUs?\n",
    "\n",
    "\n",
    "ANS = \n",
    "place each layer on a different GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
