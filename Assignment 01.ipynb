{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Explain One-Hot Encoding\n",
    "\n",
    "Ans = \n",
    "\n",
    "\n",
    "One hot encoding is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.\n",
    "\n",
    "\n",
    "One hot encoding can be defined as the essential process of converting the categorical data variables to be provided to machine and deep learning algorithms which in turn improve predictions as well as classification accuracy of a model. One Hot Encoding is a common way of preprocessing categorical features for machine learning models. This type of encoding creates a new binary feature for each possible category and assigns a value of 1 to the feature of each sample that corresponds to its original category. \n",
    "\n",
    "\n",
    "We should prefer using the One Hot Encoding method when :\n",
    "\n",
    "The categorical features present in the data is not ordinal (like the countries above)\n",
    "When the number of categorical features present in the dataset is less so that the one-hot encoding technique can be effectively applied while building the model.\n",
    "\n",
    "\n",
    "## Explain Bag of Words\n",
    "\n",
    "Ans = The bag-of-words (BOW) model is a representation that turns arbitrary text into fixed-length vectors by counting how many times each word appears. This process is often referred to as vectorization.\n",
    "\n",
    "Suppose we wanted to vectorize the following:\n",
    "\n",
    "the cat sat\n",
    "the cat sat in the hat\n",
    "the cat with the hat\n",
    "\n",
    "We’ll refer to each of these as a text document.\n",
    "\n",
    "Step 1: Determine the Vocabulary\n",
    "\n",
    "We first define our vocabulary, which is the set of all words found in our document set. The only words that are found in the 3 documents above are: the, cat, sat, in, the, hat, and with.\n",
    "Step 2: Count\n",
    "\n",
    "To vectorize our documents, all we have to do is count how many times each word appears:\n",
    "\n",
    "Now we have length-6 vectors for each document!\n",
    "\n",
    "the cat sat: [1, 1, 1, 0, 0, 0]\n",
    "the cat sat in the hat: [2, 1, 1, 1, 1, 0]\n",
    "the cat with the hat: [2, 1, 0, 0, 1, 1]\n",
    "\n",
    "Notice that we lose contextual information, e.g. where in the document the word appeared, when we use BOW. It’s like a literal bag-of-words: it only tells you what words occur in the document, not where they occurred.\n",
    "\n",
    "Despite being a relatively basic model, BOW is often used for Natural Language Processing (NLP) tasks like Text Classification. Its strengths lie in its simplicity: it’s inexpensive to compute, and sometimes simpler is better when positioning or contextual info aren’t relevant.\n",
    "\n",
    "## Explain Bag of N-Grams\n",
    "\n",
    "Ans = \n",
    "\n",
    "A bag-of-n\n",
    "\n",
    "-grams model is a way to represent a document, similar to a [bag-of-words][/terms/bag-of-words/] model.\n",
    "\n",
    "A bag-of-n\n",
    "-grams model represents a text document as an unordered collection of its n\n",
    "\n",
    "-grams.\n",
    "\n",
    "For example, let’s use the following phrase and divide it into bi-grams (n=2\n",
    "\n",
    ").\n",
    "\n",
    "    James is the best person ever.\n",
    "\n",
    "becomes\n",
    "\n",
    "    <start>James\n",
    "    James is\n",
    "    is the\n",
    "    the best\n",
    "    best person\n",
    "    person ever.\n",
    "    ever.<end>\n",
    "\n",
    "In a typical bag-of-n\n",
    "\n",
    "-grams model, these 6 bigrams would be a sample from a large number of bigrams observed in a corpus. And then James is the best person ever. would be encoded in a representation showing which of the corpus’s bigrams were observed in the sentence.\n",
    "\n",
    "A bag-of-n\n",
    "-grams model has the simplicity of the bag-of-words model, but allows the preservation of more word locality information.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Explain TF-IDF\n",
    "\n",
    "\n",
    "Ans = \n",
    "\n",
    "TF-IDF stands for Term Frequency Inverse Document Frequency of records. It can be defined as the calculation of how relevant a word in a series or corpus is to a text. The meaning increases proportionally to the number of times in the text a word appears but is compensated by the word frequency in the corpus (data-set).\n",
    "\n",
    "Term Frequency: In document d, the frequency represents the number of instances of a given word t. Therefore, we can see that it becomes more relevant when a word appears in the text, which is rational. Since the ordering of terms is not significant, we can use a vector to describe the text in the bag of term models. For each specific term in the paper, there is an entry with the value being the term frequency.\n",
    "\n",
    "Inverse Document Frequency: Mainly, it tests how relevant the word is. The key aim of the search is to locate the appropriate records that fit the demand. Since tf considers all terms equally significant, it is therefore not only possible to use the term frequencies to measure the weight of the term in the paper. First, find the document frequency of a term t by counting the number of documents containing the term.\n",
    "\n",
    "In python tf-idf values can be computed using TfidfVectorizer() method in sklearn module. \n",
    "\n",
    "Syntax:\n",
    "\n",
    "    sklearn.feature_extraction.text.TfidfVectorizer(input)\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "        input: It refers to parameter document passed, it can be a filename, file or content itself.\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "        vocabulary_: It returns a dictionary of terms as keys and values as feature indices.\n",
    "        idf_: It returns the inverse document frequency vector of the document passed as a parameter.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        fit_transform(): It returns an array of terms along with tf-idf values.\n",
    "        get_feature_names(): It returns a list of feature names.\n",
    "\n",
    "\n",
    "\n",
    "## What is OOV problem?\n",
    "\n",
    "Ans = \n",
    "\n",
    "Out-of-vocabulary (OOV) are terms that are not part of the normal lexicon found in a natural language processing environment. \n",
    "\n",
    "In speech recognition, it’s the audio signal that contains these terms. Word vectors are the mathematical equivalent of word meaning. But the limitation of word embeddings is that the words need to have been seen before in the training data.\n",
    "\n",
    "When a word that’s not in the training set occurs in real data, this causes a problem. There are various techniques to avoid a zero-probability occurrence including smoothing and replacing the word a synonym.\n",
    "\n",
    "\n",
    "\n",
    "## What are word embeddings?\n",
    "\n",
    "Ans = \n",
    "\n",
    "It is an approach for representing words and documents. Word Embedding or Word Vector is a numeric vector input that represents a word in a lower-dimensional space. It allows words with similar meaning to have a similar representation. They can also approximate meaning. A word vector with 50 values can represent 50 unique features.\n",
    "\n",
    "Features: Anything that relates words to one another. Eg: Age, Sports, Fitness, Employed etc. Each word vector has values corresponding to these features.\n",
    "\n",
    "Goal of Word Embeddings\n",
    "\n",
    "    To reduce dimensionality\n",
    "    To use a word to predict the words around it\n",
    "    Inter word semantics must be captured\n",
    "\n",
    "How are Word Embeddings used?\n",
    "\n",
    "    They are used as input to machine learning models.\n",
    "    Take the words —-> Give their numeric representation —-> Use in training or inference\n",
    "    To represent or visualize any underlying patterns of usage in the corpus that was used to train them.\n",
    "\n",
    "## Explain Continuous bag of words (CBOW)\n",
    "\n",
    "Ans = \n",
    "\n",
    "\n",
    "CBOW (Continuous Bag of Words) model predicts a target word based on the context of the surrounding words in a sentence or text. It is trained using a feedforward neural network where the input is a set of context words, and the output is the target word.\n",
    "\n",
    "The Model Architecture\n",
    "cbow\n",
    "\n",
    "The CBOW model architecture is as shown above. The model tries to predict the target word by trying to understand the context of the surrounding words. Consider the same sentence as above, ‘It is a pleasant day’.The model converts this sentence into word pairs in the form (contextword, targetword). The user will have to set the window size. If the window for the context word is 2 then the word pairs would look like this: ([it, a], is), ([is, pleasant], a),([a, day], pleasant). With these word pairs, the model tries to predict the target word considered the context words. \n",
    "\n",
    "## Explain SkipGram\n",
    "\n",
    "Ans = \n",
    "\n",
    "Skip-gram is one of the unsupervised learning techniques used to find the most related words for a given word. Skip-gram is used to predict the context word for a given target word. It's reverse of CBOW algorithm. Here, target word is input while context words are output.\n",
    "\n",
    "# Explain Glove Embeddings.\n",
    "\n",
    "Ans = \n",
    "\n",
    "The basic idea behind the GloVe word embedding is to derive the relationship between the words from statistics. Unlike the occurrence matrix, the co-occurrence matrix tells you how often a particular word pair occurs together. Each value in the co-occurrence matrix represents a pair of words occurring together.17-Aug-2021\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
